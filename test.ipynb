{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL=\"qwen2.5:14b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    llm = ChatOllama(model=LLM_MODEL)\n",
    "    return llm\n",
    "\n",
    "def query_llama(prompt):\n",
    "    llm = get_llm()\n",
    "    res = llm.invoke(prompt)\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "\n",
    "ITEMS_DICT = {\n",
    "    'signalment_physical': [\n",
    "        'age', 'breed', 'gender', 'neuter_status', 'vomit_nausea',\n",
    "        'lethargy_weakness', 'appetite_loss', 'diarrhea_melena',\n",
    "        'abdominal_pain', 'weight_loss', 'duration', 'bw', 'temp',\n",
    "        'hr', 'rr', 'bcs', 'hydration_status'\n",
    "    ],\n",
    "    'cbc': [\n",
    "        'wbc', 'red_blood_cell_count', 'hemoglobin', 'packed_cell_volume',\n",
    "        'mean_corpuscular_volume', 'mean_corpuscular_hemoglobin_concentration',\n",
    "        'plasma_protein', 'platelet_count', 'absolute_neutrophil',\n",
    "        'absolute_bands', 'absolute_lymphocyte', 'absolute_monocyte',\n",
    "        'absolute_eosinophil', 'absolute_basophil', 'absolute_other'\n",
    "    ],\n",
    "    'chem': [\n",
    "        'glucose', 'lactic_acid', 'blood_urea_nitrogen', 'creatinine',\n",
    "        'sodium', 'potassium', 'enzymatic_carbon_dioxide', 'chloride',\n",
    "        'anion_gap_calculated', 'calcium', 'phosphorus', 'magnesium',\n",
    "        'total_protein', 'albumin', 'globulin', 'total_bilirubin',\n",
    "        'gamma_glutamyltransferase', 'alanine_aminotransferase',\n",
    "        'alkaline_phosphatase', 'cholesterol'\n",
    "    ],\n",
    "    'cpli': ['spec_cpli'],\n",
    "    'aus': [\n",
    "        'size', 'echogenecity_of_pancreatic_parenchyma',\n",
    "        'echogenecity_of_peripancreatic_mesentery', 'pancreatic_echotexture',\n",
    "        'free_fluid_effusion', 'conclusions'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def create_extraction_prompt(file_name, content):\n",
    "    return f\"\"\"\n",
    "    Analyze this veterinary medical document and extract structured data in JSON format.\n",
    "    Extract the details based on the following categories: {ITEMS_DICT[file_name]}\n",
    "    Follow this exact structure:\n",
    "    ```{{\n",
    "        \"signalment_physical\": {{\n",
    "            \"age\": \"<value>\",\n",
    "            \"breed\": \"<value>\",\n",
    "            ...\n",
    "        }},\n",
    "        \"cbc\": {{\n",
    "            \"wbc\": \"<value>\",\n",
    "            ...\n",
    "        }},\n",
    "        ...\n",
    "    }}```\n",
    "\n",
    "    Extract ALL available information from this text:\n",
    "    {content}\n",
    "\n",
    "    Return ONLY the JSON with extracted values. Use empty strings for missing information.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path).split('.')[0]\n",
    "        # with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        #     content = f.read()\n",
    "        \n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert(file_path)\n",
    "        content = result.document.export_to_markdown()\n",
    "        \n",
    "        prompt = create_extraction_prompt(file_name, content)\n",
    "        response = query_llama(prompt)\n",
    "        \n",
    "        response = response.strip(\"` \\n\")\n",
    "        if response.startswith('json'):\n",
    "            response = response[4:]\n",
    "        return json.loads(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(files):\n",
    "    data = []\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            file_name = os.path.basename(file).split('.')[0]\n",
    "            extracted_data = process_file(file)\n",
    "            \n",
    "            # Build rows for this file\n",
    "            for category in ITEMS_DICT[file_name]:\n",
    "                for item in extracted_data.get(category, {}):\n",
    "                    value = extracted_data[category][item]\n",
    "                    data.append({\n",
    "                        'filename': file_name,\n",
    "                        'items': item,\n",
    "                        'results': value,\n",
    "                        'details': ''\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(data, output_file):\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[['filename', 'items', 'results', 'details']]\n",
    "    \n",
    "    df.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Extraction completed. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/dheeraj/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/aus.pdf: string indices must be integers, not 'str'\n",
      "Extraction completed. Results saved to extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "data_directory = 'data'  # Update with your data directory\n",
    "\n",
    "signalment_physical = os.path.join(data_directory, 'signalment_physical.pdf')\n",
    "cbc = os.path.join(data_directory, 'cbc.pdf')\n",
    "chem = os.path.join(data_directory, 'chem.pdf')\n",
    "cpli = os.path.join(data_directory, 'cpli.pdf')\n",
    "aus = os.path.join(data_directory, 'aus.pdf')\n",
    "\n",
    "files = [signalment_physical, cbc, chem, cpli, aus]\n",
    "\n",
    "output_csv = 'extracted_data.csv'\n",
    "data = generate_data(files)\n",
    "\n",
    "generate_csv(data, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from docling.document_converter import DocumentConverter\n",
    "\n",
    "# converter = DocumentConverter()\n",
    "# result = converter.convert(cpli)\n",
    "# print(result.document.export_to_markdown()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
